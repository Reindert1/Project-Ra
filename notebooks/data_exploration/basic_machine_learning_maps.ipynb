{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "french-baking",
   "metadata": {},
   "source": [
    "# Basic machine learning on maps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "limited-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from joblib import parallel_backend\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pickle\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import notebook_config as cfg\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 268435460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0ba763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88366950, 6)\n"
     ]
    }
   ],
   "source": [
    "data_array = np.load(\"total_classification.npy\", allow_pickle=True)\n",
    "print(data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "requested-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66275212, 5)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(data_array[:, :-1], data_array[:, -1], random_state=0)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa56d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB accurary: 0.5331072533327825\n",
      "DecisionTreeClassifier accurary: 0.6233615735277726\n"
     ]
    }
   ],
   "source": [
    "def test_trainer(name_model, model, data):\n",
    "        with parallel_backend('threading', n_jobs=-1):\n",
    "                cross_fold = KFold(n_splits=4, random_state=1, shuffle=True)\n",
    "                accuracy = cross_val_score(model, data[\"x_train\"], data[\"y_train\"], cv=cross_fold, scoring='accuracy').mean()\n",
    "                print(f\"{name_model} accurary: {accuracy}\")\n",
    "\n",
    "models = {#'MultinomalNB': MultinomialNB(),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'DecisionTreeClassifier': DecisionTreeClassifier()}\n",
    "\n",
    "data = {\"x_train\": data_array[:, :-1],\n",
    "        \"y_train\": data_array[:, -1]}\n",
    "\n",
    "for name, model in models.items():\n",
    "    test_trainer(name, model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe33063",
   "metadata": {},
   "source": [
    "The code above is an implementation for classification using k-fold cross validation.\n",
    "The only models that where used here where gaussian naive bayes and a decision tree. This is because multinominal naive bayes gives some warinings and most other classifiers aren't build to be used with close to 90 million instances. Using these kinds of classifiers would mean that training would take a very long time with k-fold cross validation. The classifiers we should try and use are the ones that include a partial_fit method, seeing as these classifiers can be trained in batches. All of the other classifiers need all of the data to make the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4e665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current: Kmeans\n",
      "Accuracy: 0.26661370568974035\n",
      "current: GaussianNB\n",
      "Accuracy: 0.533070791738314\n",
      "current: DecisionTreeClassifier\n",
      "Accuracy: 0.6220044560400315\n",
      "current: SGDClassifier\n",
      "Accuracy: 0.36372029738871076\n"
     ]
    }
   ],
   "source": [
    "def test_trainer(name_model, model, x_train, x_val, y_train, y_val, full_x):\n",
    "        with parallel_backend('threading', n_jobs=-1):\n",
    "                print(f\"current: {name_model}\")\n",
    "                model.fit(x_train, y_train)\n",
    "\n",
    "                filename = f'models/{name_model}.sav'\n",
    "                pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "                y_pred = model.predict(x_val)\n",
    "                print(\"Accuracy:\", metrics.accuracy_score(y_val, y_pred))\n",
    "\n",
    "models = {#'MultinomalNB': MultinomialNB(),\n",
    "        'Kmeans': MiniBatchKMeans(n_clusters=5, random_state=0),#, batch_size=5000, max_iter=20),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "        'SGDClassifier': SGDClassifier(max_iter=100, tol=1e-3, random_state=0)\n",
    "        }\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data_array[:, :-1], data_array[:, -1], test_size=0.3, random_state=0)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_val = scaler.transform(x_val)\n",
    "\n",
    "# x_train = x_train / 255\n",
    "# x_val = x_val / 255\n",
    "\n",
    "x_full = np.load(\"full_try.npy\", allow_pickle=True)[:, :-1] #scaler.transform(data_array[:, :-1])\n",
    "\n",
    "# print(x_full.dtype)\n",
    "# x_full = x_full.astype(\"float32\")\n",
    "# x_full = x_full / 255\n",
    "# print(x_full.dtype)\n",
    "\n",
    "classifier = Image.open(f\"{cfg.data_path}/Maps_labels.tif\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    test_trainer(name, model, x_train, x_val, y_train, y_val, x_full) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b93a7",
   "metadata": {},
   "source": [
    "Normalization seems like something that we need to do. However in out case it would be detrimental to normalize the dataset. This is because an integer between 0 and 255 can be stored in a uint8 format, while if we normalize we'll have to make use of a float32. This would mean our entire dataset would get 4 times as big in memory. In testing we found that this increase in size can cause some mayor problems.\n",
    "\n",
    "current: Kmeans \\\n",
    "Accuracy: 0.26661370568974035 \\\n",
    "current: GaussianNB \\\n",
    "Accuracy: 0.533070791738314 \\\n",
    "current: DecisionTreeClassifier \\\n",
    "Accuracy: 0.6220044560400315 \\\n",
    "current: SGDClassifier \\\n",
    "Accuracy: 0.36372029738871076 \n",
    "\n",
    "From these results it seems like simply using gaussian pyramids might not be enough to achief a good accuracy in the classification of pixels. The highest accuracy achieved was a measerly 62.2%. The decision tree also scores well but it might be overfitted seeing as it's currently running with it's default parameters. The reason we're running with default parameters here is simpy so we can try and get a classification seeing as there are a lot of errors due to the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c22e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.05310343446378796\n"
     ]
    }
   ],
   "source": [
    "# x_train, x_val, y_train, y_val = train_test_split(data_array[:, :-1], data_array[:, -1], test_size=0.99, random_state=0)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_val = scaler.transform(x_val)\n",
    "\n",
    "# with parallel_backend('threading', n_jobs=-1):\n",
    "#     clf = svm.SVC(kernel='linear', max_iter=1) \n",
    "\n",
    "#     clf.fit(x_train, y_train)\n",
    "\n",
    "#     y_pred = clf.predict(x_val)\n",
    "#     print(\"Accuracy:\", metrics.accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47523da7",
   "metadata": {},
   "source": [
    "The commented code above is code for trying to implement support vector machines (SVM). However, it seems like out dataset is too large to get a model that can be trained in a reasonable ammount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629f312",
   "metadata": {},
   "source": [
    "The code below here is all meant as test to see what would be the fastest way to also create a full image with the model. This is done due to the fact that classifying the whole image array can cause tremendes amounts of stuttering and lag due to the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b16ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current: DecisionTreeClassifier\n",
      "Accuracy: 0.6219942712367765\n"
     ]
    }
   ],
   "source": [
    "def test_trainer(name_model, model, x_train, x_val, y_train, y_val, full_x):\n",
    "        with parallel_backend('threading', n_jobs=-1):\n",
    "                print(f\"current: {name_model}\")\n",
    "                #mnb.partial_fit(x_train, y_train, classes)\n",
    "                model.fit(x_train, y_train)\n",
    "\n",
    "                filename = f'models/{name_model}.sav'\n",
    "                pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "                y_pred = model.predict(x_val)\n",
    "                print(\"Accuracy:\", metrics.accuracy_score(y_val, y_pred))\n",
    "\n",
    "                full_pred = model.predict(full_x).reshape(16384, 16384)\n",
    "                full_image = Image.fromarray(full_pred, mode=\"P\")\n",
    "                full_image.putpalette(classifier.getpalette())\n",
    "                full_image.save(f\"{name_model}.tif\")\n",
    "\n",
    "models = {#'MultinomalNB': MultinomialNB(),\n",
    "        #'Kmeans': MiniBatchKMeans(n_clusters=5, random_state=0),#, batch_size=5000, max_iter=20),\n",
    "        #'GaussianNB': GaussianNB(),\n",
    "        'DecisionTreeClassifier': DecisionTreeClassifier()\n",
    "        #'SGDClassifier': SGDClassifier(max_iter=100, tol=1e-3, random_state=0)\n",
    "        }\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data_array[:, :-1], data_array[:, -1], test_size=0.3, random_state=0)\n",
    "\n",
    "x_full = np.load(\"full_try.npy\", allow_pickle=True)[:, :-1]\n",
    "\n",
    "classifier = Image.open(f\"{cfg.data_path}/Maps_labels.tif\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    test_trainer(name, model, x_train, x_val, y_train, y_val, x_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848a7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_tif(model_file, model_name, data, palette):\n",
    "    Path(\"images\").mkdir(parents=True, exist_ok=True)\n",
    "    loaded_model = pickle.load(open(model_file, 'rb'))\n",
    "    full_pred = loaded_model.predict(data).reshape(16384, 16384)\n",
    "    full_image = Image.fromarray(full_pred, mode=\"P\")\n",
    "    full_image.putpalette(palette)\n",
    "    full_image.save(f\"images/{model_name}.tif\")\n",
    "\n",
    "classifier = Image.open(f\"{cfg.data_path}/Maps_labels.tif\")\n",
    "\n",
    "model_file = \"models/DecisionTreeClassifier.sav\"\n",
    "model_name = \"DecisionTreeClassifier\"\n",
    "x_full = np.load(\"full_try.npy\", allow_pickle=True)[:, :-1]\n",
    "palette = classifier.getpalette()\n",
    "\n",
    "# model_to_tif(model_file, model_name, x_full, palette)\n",
    "\n",
    "model_file = \"models/GaussianNB.sav\"\n",
    "model_name = \"GaussianNB\"\n",
    "model_to_tif(model_file, model_name, x_full, palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685c23c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "test = Image.open(f\"images/GaussianNB.tif\")\n",
    "test_array = np.array(test)\n",
    "unique_colors = np.unique(test_array)\n",
    "print(unique_colors)\n",
    "unique_colors_train = np.unique(y_train)\n",
    "print(unique_colors_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed753a4c",
   "metadata": {},
   "source": [
    "For some reason it seems like gaussian naive bayes never classifies anything as label 5.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
